{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b6b815-d6eb-4029-ad8f-090137fd47ef",
   "metadata": {},
   "source": [
    "## Q1. What is the purpose of Grid Search CV in machine learning, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbfdea1-b94b-4844-9109-013bd88fdd89",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Grid Search CV (Cross-Validation) is a technique used to find the best combination of hyperparameters for a machine learning model. It works by exhaustively searching over a specified parameter grid. For each combination of parameters, it performs cross-validation and evaluates performance. The combination that gives the best performance metric (e.g., accuracy, F1-score) is selected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4070b84c-9998-4732-901d-5f1bee819b61",
   "metadata": {},
   "source": [
    "## Q2. Describe the difference between Grid Search CV and Randomized Search CV, and when might you choose one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c022ed8-9595-4b51-915f-80f50a20965a",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Grid Search CV tests all possible combinations of hyperparameters, while Randomized Search CV tests only a fixed number of random combinations.\n",
    "\n",
    "Use Grid Search CV when the parameter space is small and you want to be thorough.\n",
    "\n",
    "Use Randomized Search CV when the parameter space is large or training time is limited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722b3fff-a9af-45fe-8b00-d589f411b971",
   "metadata": {},
   "source": [
    "## Q3. What is data leakage, and why is it a problem in machine learning? Provide an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe9188e-293c-4dd9-98ac-d04f85e71f1a",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Data leakage happens when information from outside the training dataset is used to create the model. This can lead to overly optimistic results during training but poor performance on real-world data.\n",
    "\n",
    "Example: Including a feature in your dataset that is derived from the target variable, like using \"Total Purchase Amount\" to predict whether someone made a purchase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d48f12-da13-4d03-9204-458d549f182d",
   "metadata": {},
   "source": [
    "## Q4. How can you prevent data leakage when building a machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4378d6-f35a-41c4-9b4f-1224fcf54144",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Always split the dataset into training and test sets before performing any data preprocessing.\n",
    "\n",
    "Apply transformations like scaling or encoding only on the training set, then apply the same transformations to the test set.\n",
    "\n",
    "Avoid using future data in the training phase in time-series problems.\n",
    "\n",
    "Be cautious with automated feature selection and leakage from target-related features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a1979a-b9ff-4836-99da-b38e9dcaa42c",
   "metadata": {},
   "source": [
    "## Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c62569a-6eb9-4023-a75c-1749196b561f",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "A confusion matrix is a table used to evaluate the performance of a classification model. It shows the number of:\n",
    "\n",
    "True Positives (TP)\n",
    "\n",
    "False Positives (FP)\n",
    "\n",
    "True Negatives (TN)\n",
    "\n",
    "False Negatives (FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9622e2-6104-48fe-89bb-748237406e57",
   "metadata": {},
   "source": [
    "## Q6. Explain the difference between precision and recall in the context of a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1ede79-2467-4b76-b5d9-c83720cc34c6",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Precision is the ratio of correctly predicted positive observations to the total predicted positives. Formula: Precision = TP / (TP + FP)\n",
    "\n",
    "Recall is the ratio of correctly predicted positive observations to all actual positives. Formula: Recall = TP / (TP + FN)\n",
    "\n",
    "Precision focuses on how accurate the positive predictions are, while recall focuses on how well the model captures actual positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ec81c9-2fb4-4b84-a6b1-82a06af36e78",
   "metadata": {},
   "source": [
    "## Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911de436-0e5f-4bdb-b68c-fe749406ac83",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "By looking at the off-diagonal values in the confusion matrix:\n",
    "\n",
    "High FP means the model is incorrectly predicting too many positives.\n",
    "\n",
    "High FN means the model is missing actual positive cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae192e3-9a0d-4c96-93b2-62c8ba11e3e7",
   "metadata": {},
   "source": [
    "## Q8. What are some common metrics that can be derived from a confusion matrix, and how are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff1bdb0-97e8-4963-9f2d-8d58e5544e67",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "Specificity = TN / (TN + FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab63d3f-a476-41b6-969c-78842ccd39da",
   "metadata": {},
   "source": [
    "## Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3335ff-9ca1-44c0-9a18-ceeadd65d237",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Accuracy depends on the number of correct predictions (TP + TN) divided by the total number of predictions. However, in imbalanced datasets, accuracy can be misleading. A confusion matrix helps break down the true positives and negatives, revealing where the model may be failing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb0c8d6-adfa-4099-96c4-bdaed91ab1b4",
   "metadata": {},
   "source": [
    "## Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c032e79-4ce6-4cdc-b0fa-3c86040f1ed8",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "By analyzing the confusion matrix, you can:\n",
    "\n",
    "Detect if the model is favoring one class over another.\n",
    "\n",
    "Identify underrepresented or misclassified classes.\n",
    "\n",
    "Understand the model's weaknesses and adjust thresholds or retrain with balanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7770e8d6-ebef-4c2c-93b6-17d0ae67e55a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
