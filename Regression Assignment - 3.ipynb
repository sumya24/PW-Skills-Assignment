{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12744602-6d09-4b7d-a182-c9de609abf61",
   "metadata": {},
   "source": [
    "## Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346f2d69-ff07-4874-a14e-dc9209fdcd68",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Ridge Regression is a type of linear regression that includes an L2 penalty term to the cost function, which helps prevent overfitting. It modifies the ordinary least squares (OLS) regression by shrinking the coefficients, especially when multicollinearity (high correlation between predictors) is present.\n",
    "\n",
    "In OLS, the goal is to minimize the sum of squared residuals, while in Ridge Regression, the goal is to minimize:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9807d6c5-6f5e-43c4-8a1a-2188a54de339",
   "metadata": {},
   "source": [
    "## Q2. What are the assumptions of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7291356a-b53d-4cb8-bfb5-6ddd4ce64686",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Ridge Regression shares most of the assumptions of linear regression:\n",
    "\n",
    "Linearity: The relationship between the predictors and the response is linear.\n",
    "\n",
    "Independence: The residuals (errors) are independent.\n",
    "\n",
    "Homoscedasticity: The variance of residuals is constant across all levels of the independent variables.\n",
    "\n",
    "Normality: The residuals are normally distributed.\n",
    "\n",
    "No perfect multicollinearity: Ridge can handle multicollinearity better than OLS, but perfect multicollinearity should still be avoided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e1433d-f57e-43bc-addf-88b29c9ea694",
   "metadata": {},
   "source": [
    "## Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0cbdc7-28f5-4719-a218-3b3074a2044b",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "The tuning parameter Œª (also called alpha) controls the strength of regularization. To select the optimal value of ùúÜ\n",
    "\n",
    "\n",
    "Use cross-validation, such as K-Fold Cross Validation.\n",
    "\n",
    "Evaluate model performance (e.g., RMSE or R¬≤) on validation data across a range of ùúÜ values.\n",
    "\n",
    "Choose the value of that minimizes the error or maximizes accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0434cc8d-8dea-4356-98b7-be1e6d8c717a",
   "metadata": {},
   "source": [
    "## Q4. Can Ridge Regression be used for feature selection? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec74582-ffec-4a95-b1cf-a597b3ca39ff",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Ridge Regression cannot perform feature selection in the strictest sense because it shrinks coefficients but does not reduce any to zero. Unlike Lasso, which can eliminate irrelevant features by setting their coefficients to zero, Ridge retains all features.\n",
    "\n",
    "However, Ridge can still be useful when:\n",
    "\n",
    "You want to reduce the influence of less important features.\n",
    "\n",
    "You need to stabilize the model in the presence of multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e876218-b59d-4849-b4ce-709b788bc563",
   "metadata": {},
   "source": [
    "## Q5. How does the Ridge Regression model perform in the presence of multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e689c86d-0276-43c6-8bfb-cc13cf9c7da6",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Ridge Regression performs very well in the presence of multicollinearity.\n",
    "\n",
    "In OLS, highly correlated predictors can lead to large, unstable coefficients.\n",
    "\n",
    "Ridge handles this by shrinking the coefficients, which stabilizes the solution and reduces variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02563ead-ee42-4d39-aafb-fd8d9d90a1f5",
   "metadata": {},
   "source": [
    "## Q6. Can Ridge Regression handle both categorical and continuous independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4184709-5993-435c-99cb-8645a120b5a9",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Yes, Ridge Regression can handle both categorical and continuous independent variables, but:\n",
    "\n",
    "Categorical variables must be converted into numerical form, usually using one-hot encoding or label encoding.\n",
    "\n",
    "After encoding, the data can be fed into the Ridge model just like any other numeric data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487660ee-1f84-45b8-9224-86271562daa9",
   "metadata": {},
   "source": [
    "## Q7. How do you interpret the coefficients of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3820eb9-40ff-4fa9-b45f-273a3530efc6",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "The coefficients in Ridge Regression represent the change in the response variable for a unit change in the predictor, assuming other predictors remain constant ‚Äî similar to OLS.\n",
    "\n",
    "However, due to the L2 penalty:\n",
    "\n",
    "Coefficients are biased (shrunken towards zero).\n",
    "\n",
    "The relative importance of predictors can still be understood, but their exact values may be smaller than in OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854a1709-b110-4c87-ad9b-14ad8537f785",
   "metadata": {},
   "source": [
    "## Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e662b0ee-a741-4d95-a031-59730299751b",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Yes, Ridge Regression can be applied to time-series data, but it needs to be done carefully.\n",
    "\n",
    "Here's how:\n",
    "\n",
    "Include lag features: Use past values (lags) of the dependent and/or independent variables as predictors.\n",
    "\n",
    "Avoid data leakage: When training, ensure future data isn‚Äôt used to predict the past.\n",
    "\n",
    "Use time-aware cross-validation, like TimeSeriesSplit instead of random K-Fold.\n",
    "\n",
    "Stationarity: Ensure the time-series is stationary or difference it appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b888b034-ded4-4eb7-95c4-313cad8b3df3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
