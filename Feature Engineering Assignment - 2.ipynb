{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb2bea6b-e40c-4c55-a7ce-74254d9cc180",
   "metadata": {},
   "source": [
    "## Q1: What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4005da-254f-45b2-afd1-2c861685d5a0",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Min-Max scaling transforms features to a fixed range, usually [0, 1] or [-1, 1]. It preserves the relationships between the original data but changes the scale of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fa2a818-b444-4fb9-a627-a4a3f2db70b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  ]\n",
      " [0.25]\n",
      " [0.5 ]\n",
      " [0.75]\n",
      " [1.  ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[10], [20], [30], [40], [50]])\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0193fc80-d571-40ad-be78-5707308185f4",
   "metadata": {},
   "source": [
    "## Q2: What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aaf75f-161b-4ce8-93fb-72764e491133",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "The Unit Vector technique (Normalization) scales data such that each data point (vector) has a length of 1. This is useful when the direction of the vector is more important than the magnitude.\n",
    "\n",
    "Difference from Min-Max:\n",
    "\n",
    "Min-Max: Scales values to a fixed range.\n",
    "\n",
    "Unit Vector: Scales to unit norm (length = 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b03a0da2-3d40-473f-bcbc-4ac472958a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.74278135 0.55708601 0.37139068]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "data = [[4, 3, 2]]\n",
    "normalizer = Normalizer()\n",
    "normalized = normalizer.fit_transform(data)\n",
    "print(normalized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7abb92e-5791-4adb-9b23-f3b76c94e621",
   "metadata": {},
   "source": [
    "## Q3: What is PCA (Principal Component Analysis), and how is it used in dimensionality reduction? Provide an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbce5a40-3e88-4933-aef9-a4033135bb2a",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "PCA is a statistical method to reduce the dimensionality of data while retaining most of the variance. It converts correlated features into a set of linearly uncorrelated components.\n",
    "\n",
    "Use Case:\n",
    "\n",
    "Reduce computation time\n",
    "\n",
    "Remove multicollinearity\n",
    "\n",
    "Visualize high-dimensional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ca0ee28-8d34-4ea0-a3a2-ca964e8ad9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.81517689]\n",
      " [-1.79187826]\n",
      " [ 0.97670137]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9]])\n",
    "pca = PCA(n_components=1)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "print(X_reduced)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c9db2d-8347-4b55-8fa0-b90dc2c3a1e1",
   "metadata": {},
   "source": [
    "## Q4: What is the relationship between PCA and Feature Extraction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251e5843-e832-4282-a6f6-09d95e0a7151",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "PCA is a feature extraction method because it creates new features (principal components) that are combinations of the original features, capturing the most significant patterns in the data.\n",
    "\n",
    "Example: Using PCA on [height, weight, age], we may extract new components like BodySizeScore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf1b2d6-4865-4e7d-acdd-1ec75a15d7fa",
   "metadata": {},
   "source": [
    "## Q5: How to use Min-Max scaling for a food delivery dataset (price, rating, delivery time)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2548b3b-bd59-45ea-8761-bf21205d3afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   price    rating  delivery_time\n",
      "0    0.5  1.000000            0.4\n",
      "1    0.0  0.000000            1.0\n",
      "2    1.0  0.571429            0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'price': [200, 150, 250],\n",
    "    'rating': [4.5, 3.8, 4.2],\n",
    "    'delivery_time': [30, 45, 20]\n",
    "})\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled, columns=df.columns)\n",
    "print(scaled_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e52e0f-ec32-4e30-acfb-0bb708de3c58",
   "metadata": {},
   "source": [
    "## Q6: How to use PCA for a stock price prediction dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d990b1a6-2266-45d6-a9a3-5a2823e1ec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assume `X` is your financial feature matrix\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=0.95)  # retain 95% variance\n",
    "X_reduced = pca.fit_transform(X_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c9fce5-b06c-4071-8981-6dcd00c33ae0",
   "metadata": {},
   "source": [
    "## Q7: Min-Max scale [1, 5, 10, 15, 20] to range [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d94c5f41-90c2-4992-8e05-baaeb77fb38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.        ]\n",
      " [-0.57894737]\n",
      " [-0.05263158]\n",
      " [ 0.47368421]\n",
      " [ 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([[1], [5], [10], [15], [20]])\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cece17-17d3-4d0e-962a-d568f8e2dfed",
   "metadata": {},
   "source": [
    "## Q8: PCA on [height, weight, age, gender, blood pressure]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816c5de0-03bf-4135-908c-40e20c414d6b",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Convert categorical (gender) to numerical.\n",
    "\n",
    "Standardize the data.\n",
    "\n",
    "Apply PCA.\n",
    "\n",
    "Choose components with cumulative variance ≥ 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ed43897-5b01-4d7a-aaa2-e610caaf8e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance: [9.45613542e-01 5.43864583e-02 4.85764838e-33]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Dummy data (numeric only)\n",
    "data = [[170, 70, 30, 1, 120], [160, 60, 25, 0, 115], [180, 80, 35, 1, 130]]\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(data_scaled)\n",
    "print(\"Explained Variance:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Select components where cumulative variance ≥ 95%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf501af-d7b8-482c-afd5-fe9d6b30a4c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
