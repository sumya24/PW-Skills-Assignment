{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d289b80a-626e-41fc-9aa1-b9d6ba6ed5c3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Question 1: What is Bayes' Theorem?\n",
    "\n",
    "Bayes' Theorem describes how to update probabilities based on new evidence. It is used in various applications,\n",
    "including spam filtering, medical diagnosis, and machine learning.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddd5685-2998-4fbf-ace8-4a1af1456181",
   "metadata": {},
   "source": [
    "##  Question 2: Formula for Bayes' Theorem\n",
    "\n",
    "The formula for Bayes' Theorem is:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "Where:\n",
    "- P(A|B) is the probability of event A given event B (posterior probability)\n",
    "- P(B|A) is the probability of event B given event A (likelihood)\n",
    "- P(A) is the prior probability of event A\n",
    "- P(B) is the probability of event B\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a00bed-2b6b-410d-8d27-2bb2cfadd8ff",
   "metadata": {},
   "source": [
    "\n",
    "## Question 3: Practical Use Cases of Bayes' Theorem\n",
    "\n",
    "Bayes' Theorem is widely used in:\n",
    "- **Medical Diagnosis** (e.g., probability of disease given test results)\n",
    "- **Spam Filtering** (e.g., probability of an email being spam given certain words)\n",
    "- **Machine Learning (Naive Bayes Classifier)**\n",
    "- **Risk Assessment & Decision Making**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ced5801-b705-4f99-affa-07718615c713",
   "metadata": {},
   "source": [
    "# Question 4: Relationship Between Bayes' Theorem and Conditional Probability\n",
    "\n",
    "Bayes' Theorem is an extension of conditional probability. Conditional probability P(A|B) represents the chance\n",
    "of event A occurring given that event B has occurred. Bayes' theorem helps in computing this probability when\n",
    "the reverse probability P(B|A) is easier to obtain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaba1f4c-4df4-4b31-8e4d-707af95e9d77",
   "metadata": {},
   "source": [
    "\n",
    "# Question 5: Choosing the Right Naive Bayes Classifier\n",
    "\n",
    "There are different types of Naive Bayes classifiers:\n",
    "- **Gaussian Naive Bayes:** Used when features are normally distributed.\n",
    "- **Multinomial Naive Bayes:** Used for discrete counts (e.g., text classification, bag-of-words model).\n",
    "- **Bernoulli Naive Bayes:** Used for binary data (e.g., spam classification with presence/absence of words).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04841ff9-755e-439e-9b95-1e92e2fc4ca0",
   "metadata": {},
   "source": [
    "## Question 6: Naive Bayes Classification Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c543705-f168-4257-9668-d335bae869b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Table:\n",
      "       X1=1  X1=2  X1=3  X2=1  X2=2  X2=3  X2=4\n",
      "Class                                          \n",
      "A         3     3     4     4     3     3     3\n",
      "B         2     2     1     2     2     2     3\n",
      "\n",
      "Posterior Probabilities:\n",
      "P(A | X1=3, X2=4) = 0.5971\n",
      "P(B | X1=3, X2=4) = 0.4029\n",
      "\n",
      "✅ Predicted Class: A\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Given frequency table\n",
    "data = {\n",
    "    'Class': ['A', 'B'],\n",
    "    'X1=1': [3, 2], 'X1=2': [3, 2], 'X1=3': [4, 1],\n",
    "    'X2=1': [4, 2], 'X2=2': [3, 2], 'X2=3': [3, 2], 'X2=4': [3, 3]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df.set_index('Class', inplace=True)\n",
    "print(\"Frequency Table:\")\n",
    "print(df)\n",
    "\n",
    "# Step 1: Compute Prior Probabilities\n",
    "prior_A = 0.5  # Given equal priors\n",
    "prior_B = 0.5\n",
    "\n",
    "# Step 2: Compute Likelihoods (P(X1=3 | A), P(X2=4 | A), etc.)\n",
    "likelihood_X1_given_A = df.loc['A', 'X1=3'] / df.loc['A'].sum()\n",
    "likelihood_X2_given_A = df.loc['A', 'X2=4'] / df.loc['A'].sum()\n",
    "\n",
    "likelihood_X1_given_B = df.loc['B', 'X1=3'] / df.loc['B'].sum()\n",
    "likelihood_X2_given_B = df.loc['B', 'X2=4'] / df.loc['B'].sum()\n",
    "\n",
    "# Step 3: Compute Posterior Probabilities\n",
    "posterior_A = likelihood_X1_given_A * likelihood_X2_given_A * prior_A\n",
    "posterior_B = likelihood_X1_given_B * likelihood_X2_given_B * prior_B\n",
    "\n",
    "# Normalize (Optional, since we are only comparing)\n",
    "total = posterior_A + posterior_B\n",
    "P_A_given_X = posterior_A / total\n",
    "P_B_given_X = posterior_B / total\n",
    "\n",
    "# Print Results\n",
    "print(\"\\nPosterior Probabilities:\")\n",
    "print(f\"P(A | X1=3, X2=4) = {P_A_given_X:.4f}\")\n",
    "print(f\"P(B | X1=3, X2=4) = {P_B_given_X:.4f}\")\n",
    "\n",
    "# Final Prediction\n",
    "predicted_class = 'A' if P_A_given_X > P_B_given_X else 'B'\n",
    "print(f\"\\n✅ Predicted Class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee0701d-b396-4a1c-8892-37853a991133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
