{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e198b981-d6a0-4b05-bbb8-77e467d03eb9",
   "metadata": {},
   "source": [
    "## Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2936da6-9fbb-446c-a359-cc9efa4f371e",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "R-squared (R²) is a statistical measure that indicates how well the independent variables explain the variation in the dependent variable. It ranges from 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e6c827-e4a2-40f3-8c7a-48057258b246",
   "metadata": {},
   "source": [
    "## Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca5bf3d-c3d7-4713-ab26-9bec3ba6f183",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Adjusted R-squared modifies the R² value by accounting for the number of predictors in the model. It penalizes the addition of irrelevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128817ef-30ef-4d4d-bc8b-d9bd013ddaac",
   "metadata": {},
   "source": [
    "## Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501a5632-5b0c-4cd9-a20c-ee158bb4ac36",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Adjusted R-squared is more appropriate when:\n",
    "\n",
    "You are comparing models with a different number of predictors.\n",
    "\n",
    "You want to prevent overfitting by penalizing the inclusion of irrelevant variables.\n",
    "\n",
    "You want a more reliable measure of model performance with multiple independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560ac751-8a77-4530-b7e8-52ae911fa562",
   "metadata": {},
   "source": [
    "## Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe72e9c5-e62c-49a1-93e6-a311325f5e5f",
   "metadata": {},
   "source": [
    "MSE (Mean Squared Error): It is the average of the squared differences between the actual and predicted values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95c9d93-b8d1-4c46-b45b-3df45e2f2ff2",
   "metadata": {},
   "source": [
    "RMSE (Root Mean Squared Error): It is the square root of the MSE and represents the error in the same unit as the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2768ee1-3c64-4b05-a59a-5fc405e2d337",
   "metadata": {},
   "source": [
    "MAE (Mean Absolute Error): It is the average of the absolute differences between the actual and predicted values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a684f71-3795-4976-bae3-351b1f553951",
   "metadata": {},
   "source": [
    "## Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2be531-c93d-4a61-90d6-c59891447a92",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "MAE (Mean Absolute Error): It is easy to interpret and less sensitive to outliers. However, it does not penalize large errors as much as MSE or RMSE.\n",
    "\n",
    "MSE (Mean Squared Error): It penalizes large errors more heavily, which can be useful if large errors are particularly bad in your context. However, it is in squared units, which makes interpretation harder.\n",
    "\n",
    "RMSE (Root Mean Squared Error): It is in the same units as the target variable and penalizes large errors, making it more interpretable than MSE. But it is still more sensitive to outliers than MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803bcb58-589a-464f-90a1-353b475fd7e6",
   "metadata": {},
   "source": [
    "## Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34096cc8-37b2-4fd3-b750-8414cb2d2a0f",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Lasso regularization (L1 regularization) adds a penalty term to the loss function that is equal to the absolute value of the coefficients. This can shrink some coefficients to exactly zero, effectively performing feature selection.\n",
    "\n",
    "Ridge regularization (L2 regularization) adds a penalty equal to the square of the coefficients. This shrinks coefficients but does not eliminate them entirely.\n",
    "\n",
    "You should prefer Lasso when you suspect that only a few features are important, and you want automatic feature selection. Ridge is better when all features are expected to contribute, and multicollinearity is a concern.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675c5214-bdb7-4edb-a483-067b9835f515",
   "metadata": {},
   "source": [
    "## Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27444d87-1c14-4d99-bffe-241db46a1cf4",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Regularized linear models add a penalty to the loss function that discourages overly complex models by shrinking the coefficients. This helps the model generalize better to unseen data, thus preventing overfitting.\n",
    "\n",
    "Example: Suppose a model fits the training data very well but performs poorly on test data. Applying Lasso or Ridge regularization will reduce the magnitude of the coefficients, making the model simpler and more robust to new data.\n",
    "\n",
    "This balance between fitting the training data and maintaining simplicity is what helps reduce overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ca5fff-7a2f-4e62-9cc8-9c038931f3b7",
   "metadata": {},
   "source": [
    "## Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab97ff4-833b-4cdf-b594-479f4afa5af2",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Regularized linear models, while powerful, do have some limitations:\n",
    "\n",
    "They assume a linear relationship between variables, which may not hold in real-world data.\n",
    "\n",
    "If the regularization parameter is too high, the model may underfit and miss important patterns.\n",
    "\n",
    "Lasso can perform poorly when there are highly correlated features, as it may arbitrarily drop one of them.\n",
    "\n",
    "They may not capture complex, non-linear relationships, where tree-based models or neural networks might perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e202415-e583-4105-9359-6e82bd9bce50",
   "metadata": {},
   "source": [
    "## Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e96d99-dfdf-4c48-b0f4-9e1c4bb6f01c",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "At first glance, Model B with a lower MAE of 8 seems to be the better choice, as it indicates smaller average prediction errors.\n",
    "\n",
    "However, RMSE penalizes larger errors more than MAE. So if Model A has a higher RMSE, it might be making some large errors, which could be undesirable depending on the context.\n",
    "\n",
    "The limitation is that neither RMSE nor MAE alone tells the full story. You need to consider both metrics and the specific problem context. For example, if large errors are particularly harmful, RMSE is more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbab58df-4ed2-4b94-ac28-a29ac2016bbf",
   "metadata": {},
   "source": [
    "## Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2bfacd-dee9-4da3-8b72-3bc1871c20a8",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "The choice between Ridge (Model A) and Lasso (Model B) depends on the data and the model's performance on validation metrics.\n",
    "\n",
    "Lasso is useful when you believe that only a few features are important, as it can shrink the coefficients of irrelevant features to zero. This helps with feature selection.\n",
    "\n",
    "Ridge is better when all features are expected to contribute and helps in handling multicollinearity. Since Ridge doesn’t reduce coefficients to zero, it retains all features.\n",
    "\n",
    "There are trade-offs:\n",
    "\n",
    "Lasso might remove useful correlated variables.\n",
    "\n",
    "Ridge might keep too many unnecessary variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cb36e1-c380-4fbc-9ccf-c328a37a3b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
