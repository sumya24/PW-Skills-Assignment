{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2657c7d8-468e-40d5-b947-b65a3581e602",
   "metadata": {},
   "source": [
    "## Q1. How does bagging reduce overfitting in decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed32e405-8fb2-4c57-91b9-88c16c503e84",
   "metadata": {},
   "source": [
    "Bagging (Bootstrap Aggregating) reduces overfitting by:\n",
    "\n",
    "Training multiple decision trees on different bootstrap samples (samples with replacement).\n",
    "\n",
    "Averaging predictions (for regression) or majority voting (for classification).\n",
    "\n",
    "This reduces variance, making the model less sensitive to noise or fluctuations in the training data.\n",
    "\n",
    "Especially useful with high-variance models like decision trees.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c796dc-013d-459e-9c9f-57cd53367c08",
   "metadata": {},
   "source": [
    "## Q2. What are the advantages and disadvantages of using different types of base learners in bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e4e0ee-670d-4c57-9533-68128c5369be",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "\n",
    "You can tailor the base learner to the problem.\n",
    "\n",
    "Complex learners like decision trees benefit from bagging due to high variance.\n",
    "\n",
    "Simpler models are faster and less prone to overfitting.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Some models (e.g., KNN, linear regression) may not benefit much from bagging.\n",
    "\n",
    "Complex models increase training time.\n",
    "\n",
    "Choosing an inappropriate base learner can hurt performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8162b979-8b34-41e4-9b68-80e72b80694b",
   "metadata": {},
   "source": [
    "## Q3. How does the choice of base learner affect the bias-variance tradeoff in bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e66ad2-408a-4792-804c-e3f4296eb4f7",
   "metadata": {},
   "source": [
    "High-variance learners (like unpruned decision trees): Bagging works best by reducing variance.\n",
    "\n",
    "Low-variance, high-bias learners (like linear regression): Bagging doesn’t help much since bias remains.\n",
    "\n",
    "Ideal base learners for bagging: those with low bias, high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d373bbd-6ca9-4ae6-a45a-20cc1c49c186",
   "metadata": {},
   "source": [
    "##  Q4. Can bagging be used for both classification and regression tasks? How does it differ in each case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d971e1a6-9346-4eef-9529-7dbde49d5897",
   "metadata": {},
   "source": [
    "Yes, bagging is used for both:\n",
    "\n",
    "Classification: Use majority voting (e.g., Random Forest Classifier).\n",
    "\n",
    "Regression: Use averaging of predictions (e.g., Random Forest Regressor).\n",
    "\n",
    "Both reduce variance and increase stability, but output type differs (class vs. number)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff6260-692a-4034-8612-1b655d1aea7e",
   "metadata": {},
   "source": [
    "## Q5. What is the role of ensemble size in bagging? How many models should be included in the ensemble?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eedd07-3b96-41f6-a16d-b539b673d936",
   "metadata": {},
   "source": [
    "More models → more stable predictions, but increased computation.\n",
    "\n",
    "After a certain size (e.g., 100-200 trees), performance gain becomes marginal.\n",
    "\n",
    "Ideal ensemble size: determined by cross-validation or when error plateaus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100980ab-00f2-4bf5-8d87-d5489bba5b81",
   "metadata": {},
   "source": [
    "## Q6. Can you provide an example of a real-world application of bagging in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6c3dbc-49d4-4d5b-a2aa-3d3a92639a82",
   "metadata": {},
   "source": [
    "Spam Detection (Classification):\n",
    "\n",
    "Use bagging with decision trees or Random Forest.\n",
    "\n",
    "Trained on email datasets (e.g., Spambase).\n",
    "\n",
    "Improves accuracy, reduces false positives.\n",
    "\n",
    "Other Examples:\n",
    "\n",
    "Medical diagnosis: Predict diseases using Random Forest.\n",
    "\n",
    "Credit scoring: Evaluate loan risk using bagged models.\n",
    "\n",
    "Customer churn prediction: Telecom companies use ensemble models to identify at-risk users.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8901f4f8-199b-4106-8a4c-f18a863b54fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96        63\n",
      "           1       0.96      0.99      0.98       108\n",
      "\n",
      "    accuracy                           0.97       171\n",
      "   macro avg       0.97      0.96      0.97       171\n",
      "weighted avg       0.97      0.97      0.97       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Random Forest (bagging)\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9687d496-d170-4e3e-8d09-477eaa653a36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
